<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<!-- saved from url=(0041)https://people.eecs.berkeley.edu/~Ojha/ -->
<html><head><meta http-equiv="Content-Type" content="text/html; charset=windows-1252">
  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
  <style type="text/css">
  /* Design Credits: Jon Barron and Abhishek Kar and Saurabh Gupta*/
  a {
  color: #1772d0;
  text-decoration:none;
  }
  a:focus, a:hover {
  color: #f09228;
  text-decoration:none;
  }
  body,td,th {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 16px;
    font-weight: 400
  }
  heading {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 19px;
    font-weight: 1000
  }
  strong {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 16px;
    font-weight: 800
  }
  strongred {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    color: 'red' ;
    font-size: 16px
  }
  sectionheading {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 22px;
    font-weight: 600
  }
  </style>
  <link rel="icon" type="image/png" href="https://people.eecs.berkeley.edu/~Ojha/images/seal_icon.png">
  <script async="" src="./Utkarsh Ojha_files/analytics.js"></script><script type="text/javascript" src="./Utkarsh Ojha_files/hidebib.js"></script>
  <title>Utkarsh Ojha</title>
  <meta name="Utkarsh Ojha&#39;s Berkeley Homepage" http-equiv="Content-Type" content="Utkarsh Ojha&#39;s Berkeley Homepage">
  <link href="./Utkarsh Ojha_files/css" rel="stylesheet" type="text/css">
  <!-- Start : Google Analytics Code -->
  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
    ga('create', 'UA-64069893-1', 'auto');
    ga('send', 'pageview');
  </script>
  <!-- End : Google Analytics Code -->
  <!-- Scramble Script by Jeff Donahue -->
  <script src="./Utkarsh Ojha_files/scramble.js"></script>
</head>

<body>
<table width="840" border="0" align="center" cellspacing="0" cellpadding="20">
  <tbody><tr><td>

<p align="center"><font size="7">Utkarsh Ojha</font><br>
    <b>Email</b>:
    <font id="email" style="display:inline;">uojha@ucdavis.edu</font>
  </p><table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
  

  <tbody><tr>
    <td width="67%" valign="middle" align="justify">
    <p>I am a second year PhD student at <a href="https://www.ucdavis.edu/">UC Davis</a> working with <a href="http://web.cs.ucdavis.edu/~yjlee/">Yong Jae Lee</a>. I graduated from <a href="http://mnnit.ac.in">MNNIT Allahabad</a> with Bachelors in Computer Science and Engineering. I'm interested in better understanding the generative models for image synthesis.</p>
	    
    </a><p align="center"><a>
    </a><a href="https://drive.google.com/file/d/1yzko1zXRuazaQOiYLH4cJcq0vpDd7sIB/view?usp=sharing">CV</a> | <a href="https://github.com/utkarshojha">Github</a> | <a href="https://scholar.google.com/citations?user=QGdSgfoAAAAJ&hl=en">Google Scholar</a> | <a href="https://www.linkedin.com/in/utkarsh-ojha-16a20b11b/">LinkedIn</a>
    </p>
    </td>

    <td width="33%"><a href="https://github.com/utkarshojha/utkarshojha.github.io/blob/master/Utkarsh%20Ojha_files/utkarsh.jpg"><img src="./Utkarsh Ojha_files/utkarsh.jpg" width="90%"></a></td>
  </tr>
</tbody></table>


<!--
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
  <tbody><tr><td>
    <sectionheading>News</sectionheading>
    <ul>
    <li> Serving as a reviewer for <a href="http://iccv2019.thecvf.com">ICCV'19</a>.  
    <li> Paper on unsupervised hierarchical disentanglement for image synthesis is accepted at <a href="cvpr2019.thecvf.com">CVPR'19</a> as <strong>oral</strong>.
    <li> Joined <a href="https://www.ucdavis.edu/">UC Davis</a> as a research scholar, working with Yong Jae Lee.</li> 
    <li> Paper on modelling universal adversarial perturbations accepted at <a href="cvpr2018.thecvf.com">CVPR'18</a>.</li>

    </ul>
  </td></tr>
</tbody></table>

-->

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
  <tbody><tr><td><sectionheading>Research</sectionheading></td></tr>
</tbody></table>
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">

  <tbody>


<tr>
    <td width="33%" valign="top" align="center"><a href="https://utkarshojha.github.io/few-shot-gan-adaptation/"><img src="./Utkarsh Ojha_files/fewshot_concept.gif" alt="sym" width="100%" style="border-style: none"></a>
    </td><td width="67%" valign="top">
      <p><a href="" id="ICMLW18">
      <img src="./Utkarsh Ojha_files/new.png" alt="[NEW]" width="6%" style="border-style: none">
      <heading>Few-shot Image Generation via Cross-domain Correspondence
</heading></a><br>
      <strong>Utkarsh Ojha</strong>, Yijun Li, Jingwan Lu, Alexei A. Efros, Yong Jae Lee, Eli Shechtman, Richard Zhang<br>
      CVPR 2021<br>
      </p>

      <div class="paper" id="fewshotgan21">
      <a href="https://arxiv.org/abs/2104.06820">arXiv</a> |   
      <a href="https://utkarshojha.github.io/few-shot-gan-adaptation/">Webpage</a> | 
      <a href="https://github.com/utkarshojha/few-shot-gan-adaptation">Github</a>	
      <br>
      <br>

      <p align="justify"> <i id="fewshotgan_abs" style="display: none;">Training generative models, such as GANs, on a target domain containing limited examples (e.g., 10) can easily result in overfitting. In this work, we seek to utilize a large source domain for pretraining and transfer the diversity information from source to target. We propose to preserve the relative similarities and differences between instances in the source via a novel cross-domain distance consistency loss. To further reduce overfitting, we present an anchor-based strategy to encourage different levels of realism over different regions in the latent space. With extensive results in both photorealistic and non-photorealistic domains, we demonstrate qualitatively and quantitatively that our few-shot model automatically discovers correspondences between source and target domains and generates more diverse and realistic images than previous methods.
<pre xml:space="preserve" style="display: none;">@inproceedings{ojha2021few-shot-gan,
  Author = {Ojha, Utkarsh and Li, Yijun and Lu, Cynthia and Efros, Alexei A. and Lee, Yong Jae and Shechtman, Eli and Zhang, Richard},
  Title = {Few-shot Image Generation via Cross-domain Correspondence
},
  Booktitle = {CVPR},
  Year = {2021}
}
</pre>
      </div>
    </td>
  </tr>

	  
	  
<tr>
    <td width="33%" valign="top" align="center"><a href="https://utkarshojha.github.io/inter-domain-gan/"><img src="./Utkarsh Ojha_files/furrycar_teaser.gif" alt="sym" width="100%" style="border-style: none"></a>
    </td><td width="67%" valign="top">
      <p><a href="" id="ICMLW18">
      <img src="./Utkarsh Ojha_files/new.png" alt="[NEW]" width="6%" style="border-style: none">
      <heading>Generating Furry Cars: Disentangling Object Shape and Appearance across Multiple Domains
</heading></a><br>
      <strong>Utkarsh Ojha</strong>, Krishna Kumar Singh, Yong Jae Lee<br>
      ICLR 2021<br>
      </p>

      <div class="paper" id="furrycar2021">
      <a href="https://arxiv.org/abs/2104.02052">arXiv</a> | 
      <a href="https://utkarshojha.github.io/inter-domain-gan/">Webpage</a> |
      <a href="https://github.com/utkarshojha/inter-domain-gan">Github</a>
      <br>
      <br>

      <p align="justify"> <i id="furrycar2021_abs" style="display: none;">We consider the novel task of learning disentangled representations of object shape and appearance across multiple domains (e.g., dogs and cars).  The goal is to learn a generative model that learns an intermediate distribution, which borrows a subset of properties from each domain, enabling the generation of images that did not exist in any domain exclusively.  This challenging problem requires an accurate disentanglement of object shape, appearance, and background from each domain, so that the appearance and shape factors from the two domains can be interchanged. We augment an existing approach that can disentangle factors within a single domain but struggles to do so across domains.  Our key technical contribution is to represent object appearance with a differentiable histogram of visual features, and to optimize the generator so that two images with the same latent appearance factor but different latent shape factors produce similar histograms. On multiple multi-domain datasets, we demonstrate our method leads to accurate and consistent appearance and shape transfer across domains.</i></p>

<pre xml:space="preserve" style="display: none;">@inproceedings{ojha-iclr2021,
  Author = {Ojha, Utkarsh and Singh, Krishna Kumar and Lee, Yong Jae},
  Title = {Generating Furry Cars: Disentangling Object Shape and Appearance across Multiple Domains
},
  Booktitle = {ICLR},
  Year = {2021}
}
</pre>
      </div>
    </td>
  </tr>

	  
	  
<tr>
    <td width="33%" valign="top" align="center"><a href="https://utkarshojha.github.io/elastic-infogan/"><img src="./Utkarsh Ojha_files/nips2020_teaser.PNG" alt="sym" width="100%" style="border-style: none"></a>
    </td><td width="67%" valign="top">
      <p><a href="" id="ICMLW18">
      
      <heading>Elastic-InfoGAN: Unsupervised Disentangled Representation Learning in Class-Imbalanced Data
</heading></a><br>
      <strong>Utkarsh Ojha</strong>, Krishna Kumar Singh, Cho-Jui Hsieh, Yong Jae Lee<br>
      NeurIPS 2020<br>
      </p>

      <div class="paper" id="elasticinfogan2020">
      <a href="https://arxiv.org/abs/1910.01112">arXiv</a> |
      <a href="https://utkarshojha.github.io/elastic-infogan">Webpage</a> |
      <a href="https://github.com/utkarshojha/elastic-infogan">Github</a> 
      <br>
      <br>

      <p align="justify"> <i id="elasticinfogan2020_abs" style="display: none;">We propose a novel unsupervised generative model that learns to disentangle object identity from other low-level aspects in class-imbalanced data. We first investigate the issues surrounding the assumptions about uniformity made by InfoGAN, and demonstrate its ineffectiveness to properly disentangle object identity in imbalanced data. Our key idea is to make the discovery of the discrete latent factor of variation invariant to identity-preserving transformations in real images, and use that as a signal to learn the appropriate latent distribution representing object identity. Experiments on both artificial (MNIST, 3D cars, 3D chairs, ShapeNet) and real-world (YouTube-Faces) imbalanced datasets demonstrate the effectiveness of our method in disentangling object identity as a latent factor of variation.</i></p>

<pre xml:space="preserve" style="display: none;">@inproceedings{ojha-arxiv2019,
  Author = {Ojha, Utkarsh and Singh, Krishna Kumar and Hsieh, Cho-Jui and Lee, Yong Jae},
  Title = {Elastic-InfoGAN: Unsupervised Disentangled Representation Learning in Class-Imbalanced Data
},
  Booktitle = {NeurIPS},
  Year = {2020}
}
</pre>
      </div>
    </td>
  </tr>

	  

<tr>
    <td width="33%" valign="top" align="center"><a href="https://arxiv.org/abs/1911.11758"><img src="./Utkarsh Ojha_files/thumb.png" alt="sym" width="100%" style="border-style: none"></a>
    </td><td width="67%" valign="top">
      <p><a href="https://arxiv.org/abs/1911.11758" id="ICMLW18">
      <heading>MixNMatch: Multifactor Disentanglement and Encoding for Conditional Image Generation
</heading></a><br>
      Yuheng Li, Krishna Kumar Singh, <strong>Utkarsh Ojha</strong>, Yong Jae Lee<br>
      <em>Computer Vision and Pattern Recognition (CVPR)</em>, 2020<br>
      </p>

      <div class="paper" id="mixmatch2019">
      <a href="https://arxiv.org/abs/1911.11758">arXiv</a> |
      <a href="https://github.com/Yuheng-Li/MixNMatch">Github</a> 
      <br>
      <br>

      <p align="justify"> <i id="mixmatch2019_abs" style="display: none;">We present MixNMatch, a conditional generative model that learns to disentangle and encode background, object pose, shape, and texture from real images with minimal supervision, for mix-and-match image generation. We build upon FineGAN, an unconditional generative model, to learn the desired disentanglement and image generator, and leverage adversarial joint image-code distribution matching to learn the latent factor encoders. MixNMatch requires bounding boxes during training to model background, but requires no other supervision. Through extensive experiments, we demonstrate MixNMatch's ability to accurately disentangle, encode, and combine multiple factors for mix-and-match image generation, including sketch2color, cartoon2img, and img2gif applications.</i></p>

<pre xml:space="preserve" style="display: none;">@inproceedings{li-arxiv2019,
  Author = {Li, Yuheng and Singh, Krishna Kumar and Ojha, Utkarsh and Lee, Yong Jae},
  Title = {MixNMatch: Multifactor Disentanglement and Encoding for Conditional Image Generation
},
  Booktitle = {arXiv:1911.11758},
  Year = {2019}
}
</pre>
      </div>
    </td>
  </tr>


<tr>
    <td width="33%" valign="top" align="center"><a href="http://krsingh.cs.ucdavis.edu/krishna_files/papers/finegan/index.html"><img src="./Utkarsh Ojha_files/finegan_teaser.png" alt="sym" width="100%" style="border-style: none"></a>
    </td><td width="67%" valign="top">
      <p><a href="http://krsingh.cs.ucdavis.edu/krishna_files/papers/finegan/index.html" id="ICMLW18">
      <heading>FineGAN: Unsupervised Hierarchical Disentanglement for Fine-Grained Object Generation and Discovery</heading></a><br>
      Krishna Kumar Singh*, <strong>Utkarsh Ojha*</strong>, Yong Jae Lee<br>
      (* equal contribution)<br>
      <em>Computer Vision and Pattern Recognition (CVPR)</em>, 2019<br>
      <strong><font color="red">Oral Presentation</font></strong>    	
      </p>

      <div class="paper" id="fingan2019">
      <a href="https://arxiv.org/abs/1811.11155">arXiv</a> |
      <a href="http://krsingh.cs.ucdavis.edu/krishna_files/papers/finegan/index.html">Webpage</a> |
      <a href="https://github.com/kkanshul/finegan">Github</a> 
      <br>
      <br>

      <p align="justify"> <i id="finegan_abs" style="display: none;">We propose FineGAN, a novel unsupervised GAN framework, which disentangles the background, object shape, and object appearance to hierarchically generate images of fine-grained object categories. To disentangle the factors without any supervision, our key idea is to use information theory to associate each factor to a latent code, and to condition the relationships between the codes in a specific way to induce the desired hierarchy. Through extensive experiments, we show that FineGAN achieves the desired disentanglement to generate realistic and diverse images belonging to fine-grained classes of birds, dogs, and cars. Using FineGAN's automatically learned features, we also cluster real images as a first attempt at solving the novel problem of unsupervised fine-grained object category discovery.</i></p>

<pre xml:space="preserve" style="display: none;">@inproceedings{singh-arxiv2018,
  Author = {Singh, Krishna and Ojha, Utkarsh and
  Lee, Yong Jae},
  Title = {FineGAN: Unsupervised Hierarchical
 	   Disentanglement for Fine-Grained 
	   Object Generation and Discovery},
  Booktitle = {arXiv:1811.11155},
  Year = {2018}
}
</pre>
      </div>
    </td>
  </tr>

  <tr>
    <td width="33%" valign="top" align="center"><a href="http://val.serc.iisc.ernet.in/nag/"><img src="./Utkarsh Ojha_files/nag_multiple.png" alt="sym" width="100%" style="border-style: none"></a>
    </td><td width="67%" valign="top">
      <p><a href="http://val.serc.iisc.ernet.in/nag/" id="CVPRW18">
      <heading>NAG: Network for Adversary Generation</heading></a><br>
      Konda Reddy Mopuri*, <strong>Utkarsh Ojha*</strong>, Utsav Garg, R. Venkatesh Babu <br>
      (*equal contribution)<br>
      <em>Computer Vision and Pattern Recognition (CVPR)</em>, 2018<br>
      </p>
	
      <div class="paper" id="cvprw18">
      <a href="https://arxiv.org/abs/1712.03390">arXiv</a> |
      <a href="http://val.serc.iisc.ernet.in/nag/">Webpage</a> |
      <a href="https://github.com/val-iisc/nag">Github</a>
      <br>
      <p align="justify"> <i id="cvprw18_abs" style="display: none;">Adversarial perturbations can pose a serious threat for deploying machine learning systems. Recent works have shown existence of image-agnostic perturbations that can fool classifiers over most natural images. Existing methods present optimization approaches that solve for a fooling objective with an imperceptibility constraint to craft the perturbations. However, for a given classifier, they generate one perturbation at a time, which is a single instance from the manifold of adversarial perturbations. Also, in order to build robust models, it is essential to explore the manifold of adversarial perturbations. In this paper, we propose for the first time, a generative approach to model the distribution of adversarial perturbations. The architecture of the proposed model is inspired from that of GANs and is trained using fooling and diversity objectives. Our trained generator network attempts to capture the distribution of adversarial perturbations for a given classifier and readily generates a wide variety of such perturbations. Our experimental evaluation demonstrates that perturbations crafted by our model (i) achieve state-of-the-art fooling rates, (ii) exhibit wide variety and (iii) deliver excellent cross model generalizability. Our work can be deemed as an important step in the process of inferring about the complex manifolds of adversarial perturbations.</i></p>

<pre xml:space="preserve" style="display: none;">@inproceedings{Mopuri_2018_CVPR,
      Author = {Reddy Mopuri, Konda and
      Ojha, Utkarsh and Garg, Utsav and
      Venkatesh Babu, R.},
      Title = {NAG: Network for 
		Adversary Generation},
      Booktitle = {The IEEE Conference on Computer
      Vision and Pattern Recognition (CVPR)},
      Year = {2018}
  }
</pre>
      </div>
    </td>
  </tr>


</tbody></table>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tbody><tr><td><br><p align="right"><font size="2">
    Template taken from <a href="http://www.cs.berkeley.edu/~barron/">here</a>
    </font></p></td></tr>
</tbody></table>

<script xml:space="preserve" language="JavaScript">
hideallbibs();
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('jpm15_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('fg15_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('wacv15_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('iclr15_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('cvpr15_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('iccv15_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('jmlr16_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('cvpr16_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('iclr16_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('cvpr17_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('icml17_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('nips17_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('iclr18_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('iclrw18_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('cvprw18_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('largeScaleCuriosity2018_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('compgan18_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('finegan2019_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('newinfogan2019_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('mixmatch2019_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('furrycar2021_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('fewshotgan_abs');
</script>




</body></html>
