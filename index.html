<!DOCTYPE html>
<html>
    <head>
        <title>Utkarsh Ojha</title>
        <link rel = "icon" type = "image/png" href = "resources/uwmlogo.png">
    </head>
    <body>
        <style>
            img {
                width: 150px;
                border-radius: 6px;
                border: 1px solid #555;
                margin-left: 20px;
                box-shadow: 0 4px 8px 0 rgba(0, 0, 0, 0.2), 0 6px 20px 0 rgba(0, 0, 0, 0.19);
                margin-top: 80px;
            }
            h1{
                font-family: Helvetica, sans-serif;
                font-size: 40px;
                font-weight: 100;
                margin-left: 200px;
                margin-top: -210px;
                color: rgb(77, 77, 77);
            }
            h1.projects{
                font-family: Helvetica, sans-serif;
                font-size: 40px;
                font-weight: 100;
                margin-left: 20px;
                margin-top: 70px;
                color: rgb(77, 77, 77);
            }

            p.intro{
                font-family: Helvetica, sans-serif;
                color: dimgray;
                margin-left: 200px;
                margin-right: 20px;
                
            }
            p.proj_desc{
                font-family: Helvetica, sans-serif;
                color: dimgray;
                margin-left: 0px;     
                margin-right: 20px;
            }
            li{
                font-family: Helvetica, sans-serif;
                color: dimgray;
                margin-left: 0px;
                
            }
            .center{
                background-color: rgb(235, 239, 247);
                width: 1000px;
                height: 2160px;
                margin: auto;
                border: 1px solid #555;
                border-radius: 6px; 
                box-shadow: 0 4px 8px 0 rgba(0, 0, 0, 0.2), 0 6px 20px 0 rgba(0, 0, 0, 0.19);
            }
            a:link {
                text-decoration: none;
                color: rgb(104, 148, 238);
            }
            a:visited {
                color: rgb(104, 148, 238);
            }
            a:hover {
                color: rgb(143, 142, 142);
            }
        </style>
        <div class="center">
        
            <img src="resources/utkarsh_bw.jpg" alt="c'mon, check the path browski">
            <h1>Utkarsh Ojha</h1>
            <p class="intro"> Postdoctoral fellow &bull; Carnegie Mellon University</p>
            <p class="intro"><a href = "mailto:uojha@wisc.edu">Email</a> &bull;  <a href="https://github.com/utkarshojha">Github</a> &bull; <a href="https://drive.google.com/file/d/1JQ6BBGRMkS0nPVwz3UKChD9w4ryX9o-M/view?usp=sharing">CV</a> &bull; <a href="https://scholar.google.com/citations?user=QGdSgfoAAAAJ&hl=en">Google scholar</a> &bull; <a href="https://drive.google.com/file/d/19LyDPscRbR0GdpWUW5Qw3CJo5cL-8flo/view?usp=sharing">Research statement</a> </p>
            <p class="intro">I am a postdoctoral researcher working with <a href="https://www.cs.cmu.edu/~ftorre/">Fernando De la Torre</a> at Carnegie Mellon University. Before this, I completed my PhD at UW Madison working with <a href="https://pages.cs.wisc.edu/~yongjaelee/">Yong Jae Lee</a>. My past work has explored generative models for computer vision tasks. In general, my philosophy is to view this class of algorithms as a tool to help us go outside the training distribution, and generate something we don't already have. Following is a list of the projects I've worked on so far (* denotes equal contribution):</p>
            <br>
            
            <ul>
                <li> <a href="https://arxiv.org/abs/2406.09400">Yo'LLaVA: Your Personalized Language and Vision Assistant
</a> &bull; NeurIPS 2024</li>
                <p class="proj_desc"><i>Thao Nguyen, Haotian Liu, Yuheng Li, Mu Cai, <b>Utkarsh Ojha</b>, Yong Jae Lee</i></p>
                <p class="proj_desc">Large multimodal models (LMMs) have gained a lot of popularity recently. But they get are trained using the publicly avaialble data on the internet. Hence, their knowledge is generic; they can recognize that a dog is present an image, but there is no way for them to recognize <i>your specific pet dog</i>. This work presents a method to take such a pretrained LMM and make it personalized using a small set of additional images, so that it can answer more personalized questions - "<i>What is <my dog> doing?</i>"</p>
                <!-- <p class="proj_desc"><b>Publication:</b> NeurIPS 2024</p> -->
            
                <br>

                
                <li> <a href="https://arxiv.org/abs/2401.10219">Edit One for All: Interactive Batch Image Editing
</a> &bull; CVPR 2024</li>
                <p class="proj_desc"><i>Thao Nguyen, <b>Utkarsh Ojha</b>, Yuheng Li, Haotian Liu, Yong Jae Lee</i></p>
                <p class="proj_desc">Most of the focus on image editing has remained on editing single images at a time. We present a method for interactive <i>batch</i> image editing, where given an edit specified by the user in the example image, our method can automatically transfer that edit to other test images so that irrespective of their initial state, they all arrive at the same final state.</p>
                <!-- <p class="proj_desc"><b>Publication:</b> CVPR 2024</p> -->
            
                <br>
                
                <li> <a href="https://arxiv.org/abs/2205.16004">What Knowledge Gets Distilled in Knowledge Distillation?</a> &bull; NeurIPS 2023</li>
                <p class="proj_desc"><i><b>Utkarsh Ojha*</b>, Yuheng Li*, Anirudh Sundara Rajan*, Yingyu Liang, Yong Jae Lee</i></p>
                <p class="proj_desc">When a student tries to mimic a teacher whle classifying an image, we see an improvement in its performance. But what happens in the background? Does the student really inherit teacher-specific properties which it would otherwise not have obtained? What are the ways in which we can study those properties? In these paper, we attempt to shed some light on this <i>dark knowledge</i> that the student inherits during the distillation process.  </p>
                <!-- <p class="proj_desc"><b>Publication:</b> NeurIPS 2023</p> --> 
                <br>
                
                <li> <a href="https://arxiv.org/abs/2307.14331">Visual Instruction Inversion: Image Editing via Visual Prompting
</a> &bull; NeurIPS 2023</li>
                <p class="proj_desc"><i>Thao Nguyen, Yuheng Li, <b>Utkarsh Ojha</b>, Yong Jae Lee</i></p>
                <p class="proj_desc">Text-conditioned image editing has emerged as a powerful tool for editing images. However, in many situations, language can be ambiguous and ineffective in describing specific image edits. We present a method for image editing via visual prompting, where given a "before" and "after" images of an edit, our goal is to learn a text-based editing direction that can be applied to unseen images.</p>
                <!-- <p class="proj_desc"><b>Publication:</b> NeurIPS 2023</p> -->
               
                
                <br>
                
                <li> <a href="https://arxiv.org/abs/2302.10174">Towards Universal Fake Image Detectors that Generalize Across Generative Models
</a> &bull; CVPR 2023</li>
                <p class="proj_desc"><i><b>Utkarsh Ojha*</b>, Yuheng Li*, Yong Jae Lee</i></p>
                <p class="proj_desc">The past few years has seen the birth of a plethora of generative models. This work attempts to build systems that can detect fake images as such across different breeds of generative models. We show why training of neural networks for real/fake classification is not a good idea, and consequently show the surprising effectiveness of a feature space <i>not</i> explicitly trained for this task.  </p>
                <!-- <p class="proj_desc"><b>Publication:</b> CVPR 2023</p> -->
                
                <br>

                <li> <a href="https://utkarshojha.github.io/few-shot-gan-adaptation/">Few-shot Image Generation via Cross-domain Correspondence</a> &bull; CVPR 2021</li>
                <p class="proj_desc"><i><b>Utkarsh Ojha</b>, Yijun Li, Jingwan Lu, Alexei A. Efros, Yong Jae Lee, Eli Shechtman, Richard Zhang<i></p>
                <p class="proj_desc">If you have 1000s of images from a domain (e.g. human faces), you can typically train a big neural network to generate images resembling its properties. What if you don't have that luxury? What if you only have, say 10 paintings from an artist, and want to <i>generate</i> more? That is the goal of this work: model a bigger distribution of a domain using extremely few training images from it.</p>  
                <!-- <p class="proj_desc"><b>Publication:</b> Computer Vision and Pattern Recognition 2021</p> -->
                
                <br>
                
                <li> <a href="https://utkarshojha.github.io/inter-domain-gan/">Generating Furry Cars: Disentangling Object Shape and Appearance across Multiple Domains</a> &bull; ICLR 2021</li>
                <p class="proj_desc"><i><b>Utkarsh Ojha</b>, Krishna Kumar Singh, Yong Jae Lee</i></p>
                <p class="proj_desc">Let's say you have data which contains images from not one, but multiple object categories (e.g. dogs and cars). Can you learn a generative model which can still disentangle object shape and its appearance? We proposed a method for this task, where upon learning such a model, we can take the appearance of a furry dog, and transfer it onto a car to create a new species of furry cars.</p>                 
                <!-- <p class="proj_desc"><b>Publication:</b> International Conference on Learning Representations 2021</p> -->
                
                <br>
                
                <li> <a href="https://utkarshojha.github.io/elastic-infogan/">Elastic-InfoGAN: Unsupervised Disentangled Representation Learning in Class-Imbalanced Data</a> &bull; NeurIPS 2020</li>
                <p class="proj_desc"><i><b>Utkarsh Ojha</b>, Krishna Kumar Singh, Yong Jae Lee</i></p>
                <p class="proj_desc">When your data has discrete object categories, a typical assumption for the discrete factors is a uniform multinomial distribution. What happens when the data has a class imbalance? We highlight the shortcomings of existing work in such scenarios, and propose a method which disentangles the discrete factor much more accurately without access to the ground-truth distribution.</p>
                <!-- <p class="proj_desc"><b>Publication:</b> Neural Information  2021</p> -->
                
                <br>
                
                <li> <a href="https://github.com/Yuheng-Li/MixNMatch">MixNMatch: Multifactor Disentanglement and Encoding for Conditional Image Generation</a> &bull; CVPR 2020</li>
                <p class="proj_desc"><i>Yuheng Li, Krishna Kumar Singh, <b>Utkarsh Ojha</b>, Yong Jae Lee</i></p>
                <p class="proj_desc">Let's say you captured two pictures, one of a red sparrow, and another of a white swan. You're feeling creative, and want to imagine how that white swan would look with that red sparrow's appearance over it. MixNMatch does precisely that: it takes in <i>real</i> images, and extracts the object's shape and appearance independently, and combine them to create a hybrid bird: a red swan.</p>
                
                <br>
                
                <li> <a href="http://krsingh.cs.ucdavis.edu/krishna_files/papers/finegan/index.html">FineGAN: Unsupervised Hierarchical Disentanglement for Fine-Grained Object Generation and Discovery</a> &bull; CVPR 2019</li>
                <p class="proj_desc"><i>Krishna Kumar Singh*, <b>Utkarsh Ojha*</b>, Yong Jae Lee</i></p>
                <p class="proj_desc">Imagine a collection of natural birds. The goal in this project was to have a model which generates realistic images, and also learns to control its different properties. For example, the proposed method learns to control object shape, appearance, pose, background - without any supervision. We could now borrow the appearance of a colorful hummingbird, and put it over the body of a seagull. </p>
                
                <br>
                
                <li> <a href="https://github.com/val-iisc/nag">NAG: Network for Adversary Generation</a> &bull; CVPR 2018</li>
                <p class="proj_desc"><i>Konda Reddy Mopuri*, <b>Utkarsh Ojha*</b>, Utsav Garg, R. Venkatesh Babu</i></p>
                <p class="proj_desc">Universal adversarial perturbation describes an image-agnostic noise pattern, which when added to any natural image will fool a neural network based classifier. We proposed a method to generate not one, but a distribution of such noise images for a neural network. These were much stronger in terms of fooling not just the targeted classifier, but also many unseen ones.</p>
                
                <br>
            </ul>
        </div>
    </body>
</html>
